{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Виртуальная стажировка Shift + Enter по направлению Data Science**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Тренировка классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset massive (/home/sergey/.cache/huggingface/datasets/AmazonScience___massive/en-US/1.0.0/71d360eb7d7a18565ff8c10609cebf714fce3cc390e173ba5b02ffd48543cdc1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e26b499e7e46449d3df81e3af2eaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# загрузка датасета\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "df = load_dataset(\"AmazonScience/massive\", \"en-US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'locale', 'partition', 'scenario', 'intent', 'utt', 'annot_utt', 'worker_id', 'slot_method', 'judgments'],\n",
       "        num_rows: 11514\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'locale', 'partition', 'scenario', 'intent', 'utt', 'annot_utt', 'worker_id', 'slot_method', 'judgments'],\n",
       "        num_rows: 2033\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'locale', 'partition', 'scenario', 'intent', 'utt', 'annot_utt', 'worker_id', 'slot_method', 'judgments'],\n",
       "        num_rows: 2974\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1',\n",
       " 'locale': 'en-US',\n",
       " 'partition': 'train',\n",
       " 'scenario': 16,\n",
       " 'intent': 48,\n",
       " 'utt': 'wake me up at nine am on friday',\n",
       " 'annot_utt': 'wake me up at [time : nine am] on [date : friday]',\n",
       " 'worker_id': '1',\n",
       " 'slot_method': {'slot': [], 'method': []},\n",
       " 'judgments': {'worker_id': [],\n",
       "  'intent_score': [],\n",
       "  'slots_score': [],\n",
       "  'grammar_score': [],\n",
       "  'spelling_score': [],\n",
       "  'language_identification': []}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# первая строчка из трейн датасета\n",
    "df[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для классификации интента нам нужны лишь тексты запросов. Выберем только колонки `intent` и `utt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 11514\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 2033\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 2974\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.select_columns(column_names=[\"intent\", \"utt\"]).rename_columns({\"intent\":\"label\", \"utt\":\"text\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взглянем подробнее на фичу `intent` из `train` сплита нашего датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 48, 'text': 'wake me up at nine am on friday'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# первая строка из трейна\n",
    "df[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# уникальные значения интента\n",
    "import numpy as np\n",
    "\n",
    "np.unique(df[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['datetime_query', 'iot_hue_lightchange', 'transport_ticket', 'takeaway_query', 'qa_stock', 'general_greet', 'recommendation_events', 'music_dislikeness', 'iot_wemo_off', 'cooking_recipe', 'qa_currency', 'transport_traffic', 'general_quirky', 'weather_query', 'audio_volume_up', 'email_addcontact', 'takeaway_order', 'email_querycontact', 'iot_hue_lightup', 'recommendation_locations', 'play_audiobook', 'lists_createoradd', 'news_query', 'alarm_query', 'iot_wemo_on', 'general_joke', 'qa_definition', 'social_query', 'music_settings', 'audio_volume_other', 'calendar_remove', 'iot_hue_lightdim', 'calendar_query', 'email_sendemail', 'iot_cleaning', 'audio_volume_down', 'play_radio', 'cooking_query', 'datetime_convert', 'qa_maths', 'iot_hue_lightoff', 'iot_hue_lighton', 'transport_query', 'music_likeness', 'email_query', 'play_music', 'audio_volume_mute', 'social_post', 'alarm_set', 'qa_factoid', 'calendar_set', 'play_game', 'alarm_remove', 'lists_remove', 'transport_taxi', 'recommendation_movies', 'iot_coffee', 'music_query', 'play_podcasts', 'lists_query'], id=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# другое представление intent\n",
    "df[\"train\"].features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datetime_query',\n",
       " 'iot_hue_lightchange',\n",
       " 'transport_ticket',\n",
       " 'takeaway_query',\n",
       " 'qa_stock',\n",
       " 'general_greet',\n",
       " 'recommendation_events',\n",
       " 'music_dislikeness',\n",
       " 'iot_wemo_off',\n",
       " 'cooking_recipe',\n",
       " 'qa_currency',\n",
       " 'transport_traffic',\n",
       " 'general_quirky',\n",
       " 'weather_query',\n",
       " 'audio_volume_up',\n",
       " 'email_addcontact',\n",
       " 'takeaway_order',\n",
       " 'email_querycontact',\n",
       " 'iot_hue_lightup',\n",
       " 'recommendation_locations',\n",
       " 'play_audiobook',\n",
       " 'lists_createoradd',\n",
       " 'news_query',\n",
       " 'alarm_query',\n",
       " 'iot_wemo_on',\n",
       " 'general_joke',\n",
       " 'qa_definition',\n",
       " 'social_query',\n",
       " 'music_settings',\n",
       " 'audio_volume_other',\n",
       " 'calendar_remove',\n",
       " 'iot_hue_lightdim',\n",
       " 'calendar_query',\n",
       " 'email_sendemail',\n",
       " 'iot_cleaning',\n",
       " 'audio_volume_down',\n",
       " 'play_radio',\n",
       " 'cooking_query',\n",
       " 'datetime_convert',\n",
       " 'qa_maths',\n",
       " 'iot_hue_lightoff',\n",
       " 'iot_hue_lighton',\n",
       " 'transport_query',\n",
       " 'music_likeness',\n",
       " 'email_query',\n",
       " 'play_music',\n",
       " 'audio_volume_mute',\n",
       " 'social_post',\n",
       " 'alarm_set',\n",
       " 'qa_factoid',\n",
       " 'calendar_set',\n",
       " 'play_game',\n",
       " 'alarm_remove',\n",
       " 'lists_remove',\n",
       " 'transport_taxi',\n",
       " 'recommendation_movies',\n",
       " 'iot_coffee',\n",
       " 'music_query',\n",
       " 'play_podcasts',\n",
       " 'lists_query']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# извлечем имена из структуры выше\n",
    "df[\"train\"].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стало понятно, что `intent` - это индексы из списка категорий.\n",
    "\n",
    "Теперь надо токенизировать датасет, чтобы модель поняла естественный язык"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализируем токенайзер с BERT (distilled)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sergey/.cache/huggingface/datasets/AmazonScience___massive/en-US/1.0.0/71d360eb7d7a18565ff8c10609cebf714fce3cc390e173ba5b02ffd48543cdc1/cache-9292b9ec0b3887b6.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sergey/.cache/huggingface/datasets/AmazonScience___massive/en-US/1.0.0/71d360eb7d7a18565ff8c10609cebf714fce3cc390e173ba5b02ffd48543cdc1/cache-f454366645a0ab6b.arrow\n",
      "Loading cached processed dataset at /home/sergey/.cache/huggingface/datasets/AmazonScience___massive/en-US/1.0.0/71d360eb7d7a18565ff8c10609cebf714fce3cc390e173ba5b02ffd48543cdc1/cache-89d4ffaa8d3fb300.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 11514\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2033\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2974\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# токенезируем все, что имеем\n",
    "# с обрезанием текста под модель и динамической подачей\n",
    "\n",
    "def utt_tokenization(data):\n",
    "    return tokenizer(data[\"text\"], \n",
    "                     truncation=True, \n",
    "                     padding=True)\n",
    "\n",
    "\n",
    "tokenized_df = df.map(utt_tokenization, batched=True)\n",
    "tokenized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее загрузим методы оценки\n",
    "\n",
    "Используемые метрики:\n",
    "- accuracy - точность модели в целом\n",
    "- precision (macro) - точность в предсказании правильных значений\n",
    "- recall (macro) - доля нахождения правильных значений\n",
    "- roc_auc (macro) - среднее значение площадей под кривыми, показывающими зависимость между чувствительностью и специфичностью для каждого класса.\n",
    "\n",
    "Чем ближе эти метрики к 1, тем лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка метрик\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\", \"multiclass\")\n",
    "recall = evaluate.load(\"recall\", \"multiclass\")\n",
    "roc_auc = evaluate.load(\"roc_auc\", \"multiclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# словари для лейблов и их индексов в обе стороны, чтобы модель разобралась, что предсказывать\n",
    "# также сохраним индексы лейблов для интентов, они понадобятся позже\n",
    "\n",
    "label2id = {v: i for i, v in enumerate(df[\"train\"].features[\"label\"].names)}\n",
    "id2label = {i: v for i, v in enumerate(df[\"train\"].features[\"label\"].names)}\n",
    "whole_labels = np.unique(df[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для вычисления метрик\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "def compute_metrics(model_pred):\n",
    "    logits, labels = model_pred\n",
    "    \n",
    "    # преобразуем выхлоп модели в предсказания, найдя индексы максимальных элементов\n",
    "    # по столбцам\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # при помощи функции активации softmax преобразуем выхлоп модели в вероятности\n",
    "    # softmax: e^(x_i)/sum(e^(x_0),...,e^(x_n)), если мне память не изменяет\n",
    "    predictions_proba = nn.functional.softmax(torch.from_numpy(logits), dim=-1)\n",
    "\n",
    "    scores = {\n",
    "        \"accuracy\":accuracy.compute(predictions=predictions, \n",
    "                                    references=labels),\n",
    "        \"precision\":precision.compute(predictions=predictions, \n",
    "                                      references=labels, \n",
    "                                      average=\"macro\"),\n",
    "        \"recall\":recall.compute(predictions=predictions, \n",
    "                                references=labels, \n",
    "                                average=\"macro\"),\n",
    "\n",
    "        # так как при глубоком обучении могут отброситься какие-то лейблы,\n",
    "        # то метод оценивания \"один против всех\" не сработает (будет жаловаться на несовпадение\n",
    "        # размерности матрицы вероятностей и референсов), поэтому применяем метод \"один против \n",
    "        # одного\" и явно задаем список лейблов, который мы сохранили в ячейке выше\n",
    "        \"roc_auc\":roc_auc.compute(prediction_scores=predictions_proba, \n",
    "                                  references=labels, \n",
    "                                  labels=whole_labels,\n",
    "                                  average=\"macro\", \n",
    "                                  multi_class=\"ovo\")\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь начинаем подготовку к обучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# инициализация модели\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(df[\"train\"].features[\"label\"].names),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# аргументы для обучения\n",
    "training_args = TrainingArguments(output_dir=\"intent-class-model\")\n",
    "\n",
    "# объект учителя\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_df[\"train\"],\n",
    "    eval_dataset=concatenate_datasets([tokenized_df[\"validation\"], tokenized_df[\"test\"]]),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начинаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/sergey/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11514\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4320\n",
      "  Number of trainable parameters = 66999612\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d3ad84281e473e8acd5689bd5354d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Saving model checkpoint to intent-class-model/checkpoint-500\n",
      "Configuration saved in intent-class-model/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1551, 'learning_rate': 4.4212962962962966e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in intent-class-model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in intent-class-model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in intent-class-model/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to intent-class-model/checkpoint-1000\n",
      "Configuration saved in intent-class-model/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.872, 'learning_rate': 3.8425925925925924e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in intent-class-model/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in intent-class-model/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in intent-class-model/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to intent-class-model/checkpoint-1500\n",
      "Configuration saved in intent-class-model/checkpoint-1500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.609, 'learning_rate': 3.263888888888889e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in intent-class-model/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in intent-class-model/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in intent-class-model/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to intent-class-model/checkpoint-2000\n",
      "Configuration saved in intent-class-model/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3349, 'learning_rate': 2.6851851851851855e-05, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in intent-class-model/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in intent-class-model/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in intent-class-model/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to intent-class-model/checkpoint-2500\n",
      "Configuration saved in intent-class-model/checkpoint-2500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3683, 'learning_rate': 2.1064814814814816e-05, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in intent-class-model/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in intent-class-model/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in intent-class-model/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to intent-class-model/checkpoint-3000\n",
      "Configuration saved in intent-class-model/checkpoint-3000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2895, 'learning_rate': 1.527777777777778e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in intent-class-model/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in intent-class-model/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in intent-class-model/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to intent-class-model/checkpoint-3500\n",
      "Configuration saved in intent-class-model/checkpoint-3500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1439, 'learning_rate': 9.490740740740741e-06, 'epoch': 2.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in intent-class-model/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in intent-class-model/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in intent-class-model/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to intent-class-model/checkpoint-4000\n",
      "Configuration saved in intent-class-model/checkpoint-4000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1723, 'learning_rate': 3.7037037037037037e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in intent-class-model/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in intent-class-model/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in intent-class-model/checkpoint-4000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2928.2046, 'train_samples_per_second': 11.796, 'train_steps_per_second': 1.475, 'train_loss': 0.5841062589927956, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4320, training_loss=0.5841062589927956, metrics={'train_runtime': 2928.2046, 'train_samples_per_second': 11.796, 'train_steps_per_second': 1.475, 'train_loss': 0.5841062589927956, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5007\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d176378eb814f8c953c47b719dccce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/626 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergey/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5203498601913452,\n",
       " 'eval_accuracy': {'accuracy': 0.889754343918514},\n",
       " 'eval_precision': {'precision': 0.8617539440575972},\n",
       " 'eval_recall': {'recall': 0.8640891083206289},\n",
       " 'eval_roc_auc': {'roc_auc': 0.992628631348454},\n",
       " 'eval_runtime': 82.9724,\n",
       " 'eval_samples_per_second': 60.345,\n",
       " 'eval_steps_per_second': 7.545,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение завершено. Длилось около 45-50 минут.\n",
    "\n",
    "Метрики хорошие: roc_auc близок к единице; precision, recall, accuracy более 0,85.\n",
    "\n",
    "Сохраним модель локально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to final_model_trained\n",
      "Configuration saved in final_model_trained/config.json\n",
      "Model weights saved in final_model_trained/pytorch_model.bin\n",
      "tokenizer config file saved in final_model_trained/tokenizer_config.json\n",
      "Special tokens file saved in final_model_trained/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"final_model_trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Итоги**\n",
    "\n",
    "Мы обучили модель-трансформер DistilledBERT распознавать интенты с довольно высокой точностью\n",
    "\n",
    "Используем первую строку из тестовой выборки для демонстрации инференса с помощью [`pipeline()`](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/pipelines#transformers.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'alarm_set', 'score': 0.996651828289032}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# для подавления вывода при загрузке модели\n",
    "import logging\n",
    "logging.disable(logging.INFO)\n",
    "\n",
    "# загрузка предобученной модели из локальной директории\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./final_model_trained/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./final_model_trained/\")\n",
    "\n",
    "# загрузка пайплайна и предскзание\n",
    "pipe = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "pipe.predict(df[\"test\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить, модель выдает класс интента, который подходит по её мнению, и скор, который отображает, насколько интент предложения похож на интент из обучающей выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.\n",
    "\n",
    "Что касается улучшения модели для учета out-of-scope запросов, есть две опции по обнаружению таких запросов:\n",
    "- добавить дополнительную модель поверх - бинарный классификатор - которая распознает, out-of-scope ли запрос или нет\n",
    "- вынести out-of-scope запросы в отдельный класс и использовать ту же модель\n",
    "\n",
    "Для обоих методов, конечно, потребуются дополнительные данные, помеченные out-of-scope меткой, причем довольно много:\n",
    "- неразборчивый текст/несвязный набор слов или букв\n",
    "- запросы, не имеющие отношения к какой-либо теме (пустой диалог, например)\n",
    "- запросы с неизвестным пока еще интентом\n",
    "\n",
    "\n",
    "Насчет пользователей. Можно просто оповестить их сообщением \"не могу удовлетворить ваш запрос\" и прочие вариации. Однако есть идея поинтереснее: для улучшения пользовательского экспириенса, можно разбить out-of-scope класс на подклассы (по типу перечисления вариантов данных выше) и, исходя из подкласса, выдавать реакцию. Если смысл запроса не понятен вообще, то можно выдать просьбу о повторе запроса. Если смысл понятен, но не несет какого-либо интента, то можно продумать различные фразы с предложением какого-либо другого интента. Например: \"не понимаю, о чем идет речь, но могу поставить будильник на завтра, чтобы вы не проспали\". Если запрос неизвестен, то можно сделать так: извиниться и отправить специальную форму, где пользователь сам поставит метку своему запросу. Эта форма потом отправляется в data science отдел, где собираются дополнительные данные, и учитывается в последующем дообучении\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
